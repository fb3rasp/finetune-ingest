# Enhanced Fine-tuning Framework

A comprehensive, production-ready framework for fine-tuning large language models using LoRA (Low-Rank Adaptation) with advanced features including hardware optimization, robust error handling, and comprehensive monitoring.

## ğŸŒŸ Key Features

### âœ… **Hardware-Aware Optimization**

- Automatic GPU memory detection and optimization
- Progressive fallback strategies for model loading
- Quantization support with automatic fallbacks
- Optimal batch size and gradient accumulation

### âœ… **Robust Data Handling**

- Comprehensive dataset validation
- Template format validation (Alpaca, ChatML, Vicuna)
- Data quality checks and preprocessing
- Detailed dataset statistics and analysis

### âœ… **Enhanced Model Management**

- Progressive model loading with fallbacks (quantized â†’ full precision â†’ CPU)
- Architecture-specific LoRA configurations
- Automatic target module detection
- Memory usage estimation

### âœ… **Advanced Monitoring**

- Real-time training metrics and GPU monitoring
- Comprehensive logging with multiple levels
- Training callbacks with performance tracking
- Automatic checkpoint management

### âœ… **Model Evaluation**

- Automated response quality assessment
- Template adherence checking
- Performance benchmarking
- Consistency analysis

### âœ… **Configuration Management**

- YAML-based configuration with validation
- Command-line overrides
- Environment variable support
- Dry-run validation

## ğŸš€ Quick Start

### 1. Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Set up environment (optional)
cp config.env .env
# Edit .env to add your HUGGINGFACE_TOKEN if needed
```

### 2. Basic Usage

```bash
# Basic fine-tuning with default settings
python finetune.py

# Use custom configuration
python finetune.py --config my_config.yaml

# Validate configuration without training
python finetune.py --dry-run

# Resume from checkpoint
python finetune.py --resume-from ./policy-chatbot-v1/checkpoint-50
```

### 3. Configuration

Edit `config.yaml` to customize your training:

```yaml
model:
  base_model: "meta-llama/Llama-3.2-1B-Instruct"
  output_dir: "./my-custom-model"
  max_seq_length: 512

training:
  num_epochs: 3
  learning_rate: 2e-4
  save_steps: 25

data:
  dataset_path: "training_dataset.jsonl"
  template_format: "alpaca"

hardware:
  auto_optimize: true
  quantization:
    enabled: true
```

## ğŸ“ Project Structure

```
finetune-model/
â”œâ”€â”€ src/                          # Enhanced framework modules
â”‚   â”œâ”€â”€ config/                   # Configuration management
â”‚   â”‚   â”œâ”€â”€ model_config.py       # Model and training configs
â”‚   â”‚   â””â”€â”€ hardware_config.py    # Hardware optimization
â”‚   â”œâ”€â”€ data/                     # Data handling
â”‚   â”‚   â”œâ”€â”€ dataset_loader.py     # Dataset loading and validation
â”‚   â”‚   â””â”€â”€ preprocessor.py       # Template validation
â”‚   â”œâ”€â”€ models/                   # Model management
â”‚   â”‚   â”œâ”€â”€ model_loader.py       # Robust model loading
â”‚   â”‚   â””â”€â”€ lora_config.py        # LoRA configuration
â”‚   â”œâ”€â”€ training/                 # Training components
â”‚   â”‚   â”œâ”€â”€ trainer.py           # Enhanced trainer
â”‚   â”‚   â””â”€â”€ callbacks.py         # Training callbacks
â”‚   â””â”€â”€ utils/                    # Utilities
â”‚       â”œâ”€â”€ logging.py           # Advanced logging
â”‚       â””â”€â”€ validation.py        # Model evaluation
â”œâ”€â”€ finetune.py                   # Main training script
â”œâ”€â”€ config.yaml                   # Configuration file
â”œâ”€â”€ training_dataset.jsonl        # Training data
â”œâ”€â”€ chat.py                       # Chat interface (unchanged)
â””â”€â”€ requirements.txt              # Dependencies
```

## ğŸ”§ Advanced Usage

### Custom Configuration

```yaml
# config.yaml - Complete example
model:
  base_model: "meta-llama/Llama-3.2-1B-Instruct"
  output_dir: "./policy-chatbot-v1"
  max_seq_length: 512
  trust_remote_code: true

training:
  num_epochs: 3
  learning_rate: 2e-4
  warmup_ratio: 0.03
  save_steps: 25
  logging_steps: 5
  weight_decay: 0.001
  max_grad_norm: 0.3
  lr_scheduler_type: "cosine"
  save_total_limit: 3
  gradient_checkpointing: true

lora:
  r: 16
  alpha: 32
  dropout: 0.1
  bias: "none"
  task_type: "CAUSAL_LM"

data:
  dataset_path: "training_dataset.jsonl"
  validation_split: 0.0
  max_text_length: 2048
  template_format: "alpaca"

hardware:
  auto_optimize: true
  force_cpu: false
  mixed_precision: "bf16"
  quantization:
    enabled: true
    load_in_4bit: true
    bnb_4bit_quant_type: "nf4"
    bnb_4bit_compute_dtype: "bfloat16"
    bnb_4bit_use_double_quant: true

evaluation:
  run_evaluation: true
  test_questions:
    - "What is our company policy on remote work?"
    - "How do I request time off?"

logging:
  level: "INFO"
  log_to_file: true
  log_hardware_info: true
  log_training_metrics: true
```

### Command Line Options

```bash
# Dry run - validate config without training
python finetune.py --dry-run

# Override output directory
python finetune.py --output-dir ./my-model

# Disable evaluation
python finetune.py --no-evaluation

# Verbose logging
python finetune.py --verbose

# Resume training
python finetune.py --resume-from ./model/checkpoint-100
```

### Hardware Optimization

The framework automatically optimizes for your hardware:

- **< 8GB GPU**: 1 batch size, 8 grad accumulation, FP16
- **8-12GB GPU**: 2 batch size, 4 grad accumulation, BF16
- **12-16GB GPU**: 4 batch size, 2 grad accumulation, BF16
- **> 16GB GPU**: 8+ batch size, 1 grad accumulation, BF16

### Dataset Validation

Your training data is automatically validated for:

- Required format (JSONL with "text" field)
- Template adherence (Alpaca, ChatML, Vicuna)
- Data quality (empty texts, length limits)
- Encoding issues

## ğŸ“Š Monitoring and Logging

### Training Logs

Comprehensive logs are saved to `{output_dir}/logs/`:

- `latest.log` - Latest training log
- `training_metrics.json` - Detailed metrics
- `training_summary.json` - Training summary

### Real-time Monitoring

During training, monitor:

- Loss progression and learning rate
- GPU memory usage and utilization
- Training speed (steps/second)
- Hardware performance metrics

### Model Evaluation

Automatic evaluation includes:

- Response quality assessment
- Template adherence checking
- Inference performance metrics
- Memory usage analysis
- Consistency checking

## ğŸ› ï¸ Troubleshooting

### Common Issues

1. **CUDA Out of Memory**

   ```bash
   # The framework auto-optimizes, but you can force CPU:
   # In config.yaml:
   hardware:
     force_cpu: true
   ```

2. **Model Loading Fails**

   ```bash
   # Framework has automatic fallbacks:
   # Quantized â†’ Full Precision â†’ CPU
   # Check logs for which strategy worked
   ```

3. **Dataset Validation Errors**

   ```bash
   # Run dry run to see specific issues:
   python finetune.py --dry-run
   ```

4. **Template Format Issues**
   ```bash
   # Ensure your data follows the expected format:
   # For Alpaca (default):
   {
     "text": "Below is an instruction...\n\n### Instruction:\nYour question\n\n### Response:\nYour answer"
   }
   ```

### Performance Optimization

1. **For Faster Training**:

   - Increase batch size if you have GPU memory
   - Reduce sequence length if possible
   - Use BF16 instead of FP16 on modern GPUs

2. **For Better Quality**:

   - Increase LoRA rank (r parameter)
   - Train for more epochs
   - Use larger learning rate with warmup

3. **For Memory Efficiency**:
   - Enable gradient checkpointing
   - Use quantization
   - Reduce batch size

## ğŸ”„ Migration from Original Script

To migrate from the original `finetune.py`:

1. **Backup your data**: Your `training_dataset.jsonl` works unchanged
2. **Update dependencies**: `pip install -r requirements.txt`
3. **Create config**: Use `config.yaml` instead of hardcoded values
4. **Run**: `python finetune.py` (same basic usage)

### Key Differences

| Original             | Enhanced                        |
| -------------------- | ------------------------------- |
| Hardcoded config     | YAML configuration              |
| Basic error handling | Robust fallbacks                |
| Minimal logging      | Comprehensive monitoring        |
| Manual optimization  | Auto hardware optimization      |
| No validation        | Dataset validation              |
| Basic training       | Advanced trainer with callbacks |

## ğŸ“ˆ Results and Output

After training completes, you'll find:

```
policy-chatbot-v1/
â”œâ”€â”€ final/                    # Final model adapter
â”‚   â”œâ”€â”€ adapter_config.json
â”‚   â”œâ”€â”€ adapter_model.safetensors
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â””â”€â”€ training_config.json
â”œâ”€â”€ logs/                     # Training logs
â”‚   â”œâ”€â”€ latest.log
â”‚   â”œâ”€â”€ training_metrics.json
â”‚   â””â”€â”€ training_summary.json
â”œâ”€â”€ evaluation/               # Evaluation results (if enabled)
â”‚   â””â”€â”€ evaluation_results_*.json
â””â”€â”€ checkpoint-*/            # Training checkpoints
```

## ğŸ¤ Contributing

The enhanced framework is modular and extensible:

- Add new model architectures in `src/models/lora_config.py`
- Add new template formats in `src/data/preprocessor.py`
- Extend evaluation metrics in `src/utils/validation.py`
- Add hardware profiles in `src/config/hardware_config.py`

## ğŸ“„ License

Same as the original project - see main repository for license details.

---

**Need help?** Check the logs in `{output_dir}/logs/` for detailed information about any issues.
