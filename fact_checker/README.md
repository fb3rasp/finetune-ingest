# Fact Checker

AI-powered validation system for ensuring the quality and accuracy of generated training data.

## 🎯 **Purpose**

The Fact Checker validates Q&A training pairs generated by the Training Data Generator, ensuring factual accuracy, completeness, and consistency before fine-tuning. It provides detailed scoring and filtering capabilities to maintain high-quality training datasets.

## ✨ **Features**

### **Comprehensive Validation**

- **Factual Accuracy**: Verifies answers against source text
- **Completeness**: Ensures answers adequately address questions
- **Consistency**: Checks for internal contradictions
- **Multi-criteria Scoring**: 0-10 scale with weighted overall scores

### **Quality Assurance**

- **Automated Filtering**: Remove low-quality Q&A pairs based on thresholds
- **Detailed Reports**: Comprehensive validation results with issue tracking
- **Priority Flagging**: High/medium/low priority issue classification
- **Cross-Reference Index**: Easy lookup of validation results

### **LLM Provider Support**

- **OpenAI**: GPT models for validation
- **Anthropic**: Claude models
- **Google**: Gemini models
- **Local/Ollama**: Self-hosted models

## 🚀 **Quick Start**

### **Basic Validation**

```bash
cd fact_checker

# Validate training data with default settings
python src/validate_qa.py \
  --input /data/results/training_data.json \
  --output /data/results/validation_report.json
```

### **Advanced Validation with Filtering**

```bash
# Validate and create filtered dataset
python src/validate_qa.py \
  --input /data/results/training_data.json \
  --output /data/results/validation_report.json \
  --filtered-output /data/results/training_data_filtered.json \
  --threshold 8.0 \
  --filter-threshold 7.0 \
  --verbose
```

### **Local Model Validation**

```bash
# Use local Ollama model for validation
python src/validate_qa.py \
  --input /data/results/training_data.json \
  --output /data/results/validation_report.json \
  --validator-provider local \
  --validator-model qwen3:14b \
  --ollama-base-url http://192.168.50.133:11434
```

## 📊 **Validation Metrics**

### **Scoring System (0-10 scale)**

- **Factual Accuracy** (50% weight): Answer supported by source text
- **Completeness** (30% weight): Question adequately addressed
- **Consistency** (20% weight): No contradictions or inconsistencies

### **Validation Status**

- **PASS**: Overall score ≥ 8.0 AND factual accuracy ≥ 8
- **NEEDS_REVIEW**: Overall score 6.0-7.9 OR factual accuracy 5-7
- **FAIL**: Overall score < 6.0 OR factual accuracy < 5

### **Quality Guidelines**

- **9-10**: Excellent - Fully accurate, complete, and consistent
- **7-8**: Good - Mostly accurate with minor issues
- **5-6**: Fair - Some accuracy issues or incomplete
- **3-4**: Poor - Significant inaccuracies or contradictions
- **0-2**: Fail - Majorly incorrect or unsupported

## 🔧 **Configuration**

### **Environment Variables**

```bash
# Validation Settings
VALIDATOR_PROVIDER=local
VALIDATOR_MODEL=qwen3:14b
VALIDATOR_TEMPERATURE=0.1
VALIDATOR_THRESHOLD=8.0
VALIDATOR_FILTER_THRESHOLD=7.0
VALIDATOR_BATCH_SIZE=10

# Input/Output Paths
VALIDATOR_INPUT=/data/results/training_data.json
VALIDATOR_OUTPUT=/data/results/training_data_validation_report.json
VALIDATOR_FILTERED_OUTPUT=/data/results/training_data_filtered.json

# Display Options
VALIDATOR_QUIET=false
VALIDATOR_SHOW_DETAILS=false
VALIDATOR_SHOW_FAILED_ONLY=false

# Local Model Settings
OLLAMA_BASE_URL=http://192.168.50.133:11434
```

### **Command Line Arguments**

```bash
# Input/Output
--input, -i              Input training data JSON file
--output, -o             Output validation report JSON file
--filtered-output        Output filtered training data (removes low-scoring pairs)

# Validation Configuration
--validator-provider     LLM provider: openai, claude, gemini, local
--validator-model        Model name for validation
--validator-api-key      API key for commercial providers
--temperature            LLM temperature (default: 0.1)
--threshold              Pass/fail threshold (default: 8.0)
--filter-threshold       Filtering threshold (default: 7.0)
--batch-size             Batch size for processing (default: 10)

# Display Options
--quiet, -q              Suppress detailed output
--verbose, -v            Enable detailed progress logging

# Local Model Options
--ollama-base-url        Ollama server URL
```

## 📋 **Output Reports**

### **Validation Report Structure**

```json
{
  "validation_metadata": {
    "validator_model": "local:qwen3:14b",
    "validation_timestamp": "2025-08-23T15:30:00",
    "total_qa_pairs": 5849,
    "validation_threshold": 8.0,
    "source_file": "/data/results/training_data.json"
  },
  "summary_statistics": {
    "total_qa_pairs": 5849,
    "pass_count": 4523,
    "needs_review_count": 982,
    "fail_count": 344,
    "pass_rate": 0.773,
    "average_scores": {
      "overall": 7.8,
      "factual_accuracy": 8.1,
      "completeness": 7.6,
      "consistency": 7.9
    }
  },
  "flagged_issues": {
    "high_priority": [...],
    "medium_priority": [...],
    "low_priority": [...]
  },
  "validation_results": [...],
  "cross_reference_index": {...}
}
```

### **Individual Validation Results**

Each Q&A pair receives detailed scoring:

```json
{
  "qa_pair_id": "nzism_chunk42_qa15",
  "qa_pair_index": 314,
  "question": "What are baseline controls in NZISM?",
  "answer": "Baseline controls are minimum acceptable levels...",
  "validation_score": {
    "factual_accuracy_score": 9,
    "completeness_score": 8,
    "consistency_score": 9,
    "overall_score": 8.7,
    "issues_found": [],
    "recommendations": ["Consider adding examples"],
    "validation_status": "PASS"
  },
  "processing_time": 2.3
}
```

## 🎯 **Use Cases**

### **Quality Assurance Pipeline**

```bash
# Step 1: Generate training data
./run_split_workflow.sh

# Step 2: Validate quality
cd fact_checker
python src/validate_qa.py \
  --input /data/results/training_data.json \
  --output /data/results/validation_report.json \
  --verbose

# Step 3: Filter low-quality pairs
python src/validate_qa.py \
  --input /data/results/training_data.json \
  --filtered-output /data/results/high_quality_training_data.json \
  --filter-threshold 7.5
```

### **Iterative Improvement**

```bash
# Identify problem areas
python src/validate_qa.py \
  --input /data/results/training_data.json \
  --output /data/results/validation_report.json \
  --threshold 9.0 \
  --verbose

# Review flagged issues in validation_report.json
# Adjust Q&A generation parameters
# Re-run validation to verify improvements
```

### **Production Quality Control**

```bash
# Strict validation for production datasets
python src/validate_qa.py \
  --input /data/results/training_data.json \
  --output /data/results/production_validation.json \
  --filtered-output /data/results/production_training_data.json \
  --threshold 8.5 \
  --filter-threshold 8.0 \
  --validator-provider openai \
  --validator-model gpt-4
```

## 📈 **Performance Monitoring**

### **Progress Tracking**

- Real-time validation progress with batch processing
- Detailed logging in verbose mode
- Summary statistics every 10-100 processed pairs
- Processing time tracking per validation

### **Quality Metrics**

- Pass rate percentage
- Average scores across all criteria
- Issue distribution analysis
- Processing efficiency metrics

## 🔍 **Troubleshooting**

### **Common Issues**

1. **Low Pass Rates**:

   - Check source text quality and completeness
   - Adjust validation thresholds
   - Review Q&A generation prompts

2. **API Rate Limits**:

   - Reduce batch size with `--batch-size`
   - Use local models for unlimited processing
   - Implement delays between requests

3. **Memory Issues**:
   - Process in smaller batches
   - Use lighter local models
   - Monitor system resources

### **Quality Improvement Tips**

- **High Factual Accuracy**: Ensure source text is preserved in Q&A pairs
- **Better Completeness**: Review question complexity and answer depth
- **Improved Consistency**: Check for contradictory information in source documents
- **Overall Quality**: Balance question difficulty with answer accuracy

## 🔄 **Integration**

### **Workflow Integration**

```bash
# Complete pipeline with validation
./run_split_workflow.sh
cd fact_checker && python src/validate_qa.py --input /data/results/training_data.json
cd ../finetune-model && python finetune.py --dataset-path "/data/results/training_data_filtered.json"
```

### **Custom Validation Workflows**

The validator can be imported and used programmatically:

```python
from fact_checker.src.langchain_processing.qa_validator import QAValidator

validator = QAValidator(
    provider='local',
    model='qwen3:14b',
    validation_threshold=8.0,
    verbose=True
)

report = validator.validate_training_data(
    training_data_path='/data/results/training_data.json',
    output_path='/data/results/validation_report.json'
)
```

## 📚 **Dependencies**

```bash
# Install requirements
pip install -r requirements.txt
```

**Key Dependencies:**

- `langchain`: LLM integration framework
- `pydantic`: Data validation and parsing
- `openai`, `anthropic`, `google-generativeai`: Commercial LLM APIs
- `ollama`: Local model support

## 🎉 **Benefits**

- **Quality Assurance**: Ensure high-quality training data before fine-tuning
- **Cost Optimization**: Filter out poor examples to reduce training time
- **Iterative Improvement**: Identify and fix systematic issues in data generation
- **Transparency**: Detailed scoring and issue tracking for full visibility
- **Flexibility**: Multiple LLM providers and configurable thresholds
