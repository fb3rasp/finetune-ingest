# Document Analysis Q&A Generation System - Environment Configuration
# Copy this file to '.env' and fill in your API keys

# ============================================================================
# LLM Provider API Keys
# ============================================================================

# OpenAI Configuration
# Get your API key from: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Anthropic Claude Configuration  
# Get your API key from: https://console.anthropic.com/
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# Google Gemini Configuration
# Get your API key from: https://aistudio.google.com/app/apikey
GOOGLE_API_KEY=your_google_api_key_here

# ============================================================================
# Optional Configuration
# ============================================================================

# Default LLM Provider (if not specified via CLI)
# Options: openai, claude, gemini, local
DEFAULT_PROVIDER=openai

# Default Models (if not specified via CLI)
DEFAULT_OPENAI_MODEL=gpt-4
DEFAULT_CLAUDE_MODEL=claude-3-5-sonnet-20241022
DEFAULT_GEMINI_MODEL=gemini-pro
DEFAULT_LOCAL_MODEL=llama3

# ============================================================================
# Processing Configuration (optional - can be overridden via CLI)
# ============================================================================

# Document Processing
DEFAULT_CHUNK_SIZE=1000
DEFAULT_CHUNK_OVERLAP=200
DEFAULT_QUESTIONS_PER_CHUNK=3

# LLM Parameters
DEFAULT_TEMPERATURE=0.7
DEFAULT_MAX_TOKENS=2000

# File Paths
DEFAULT_INCOMING_DIR=./incoming
DEFAULT_OUTPUT_FILE=./training_data.json

# ============================================================================
# Local Model Configuration (Ollama)
# ============================================================================

# Ollama API endpoint (usually localhost)
OLLAMA_BASE_URL=http://localhost:11434

# ============================================================================
# Advanced Settings
# ============================================================================

# Enable debug logging
DEBUG_MODE=false

# Enable batch processing for large document collections
ENABLE_BATCH_PROCESSING=false

# Maximum concurrent LLM requests (for batch processing)
MAX_CONCURRENT_REQUESTS=3

# ============================================================================
# Usage Instructions
# ============================================================================
# 
# 1. Copy this file to '.env': cp config_example.env .env
# 2. Edit '.env' and add your API keys
# 3. Activate your conda environment: conda activate finetune
# 4. Run the system: python src/main.py --provider claude
#
# For more details, see the README.md file
# ============================================================================ 