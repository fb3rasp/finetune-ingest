#!/usr/bin/env python3
"""
Convert Q&A JSON files to training dataset JSONL format.

This script converts Q&A files generated by the training data generator 
(which have 'training_pairs' structure) to the Alpaca-style JSONL format
used for model fine-tuning.
"""

import json
import os
import sys
import argparse
from pathlib import Path
from typing import Dict, List

# Add project root to path for importing common modules
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.abspath(os.path.join(CURRENT_DIR, '..', '..'))
sys.path.insert(0, PROJECT_ROOT)

from common.utils.helpers import log_message


def convert_qa_to_training_format(qa_data: Dict) -> List[str]:
    """
    Convert Q&A data to Alpaca-style training format.
    
    Args:
        qa_data: Dictionary containing training_pairs and metadata
        
    Returns:
        List of JSONL strings in Alpaca format
    """
    training_lines = []
    
    if "training_pairs" not in qa_data:
        log_message("No 'training_pairs' found in Q&A data")
        return training_lines
    
    for pair in qa_data["training_pairs"]:
        if "question" not in pair or "answer" not in pair:
            log_message(f"Skipping invalid Q&A pair: {pair}")
            continue
            
        # Create Alpaca-style prompt
        alpaca_text = (
            "Below is an instruction that describes a task. "
            "Write a response that appropriately completes the request.\n\n"
            f"### Instruction:\n{pair['question']}\n\n"
            f"### Response:\n{pair['answer']}"
        )
        
        # Create JSONL entry
        jsonl_entry = {"text": alpaca_text}
        training_lines.append(json.dumps(jsonl_entry, ensure_ascii=False))
    
    return training_lines


def process_qa_file(input_file: Path, output_file: Path) -> Dict[str, int]:
    """
    Process a single Q&A JSON file and convert to training format.
    
    Args:
        input_file: Path to input Q&A JSON file
        output_file: Path to output training JSONL file
        
    Returns:
        Dictionary with processing statistics
    """
    try:
        log_message(f"Processing: {input_file.name}")
        
        # Load Q&A data
        with open(input_file, 'r', encoding='utf-8') as f:
            qa_data = json.load(f)
        
        # Convert to training format
        training_lines = convert_qa_to_training_format(qa_data)
        
        if not training_lines:
            log_message(f"No training data generated from {input_file.name}")
            return {"processed": 0, "skipped": 1}
        
        # Write training data
        with open(output_file, 'w', encoding='utf-8') as f:
            for line in training_lines:
                f.write(line + '\n')
        
        log_message(f"Created {output_file.name} with {len(training_lines)} training examples")
        
        return {"processed": len(training_lines), "skipped": 0}
        
    except Exception as e:
        log_message(f"Error processing {input_file.name}: {e}")
        return {"processed": 0, "skipped": 1}


def generate_output_filename(input_file: Path) -> str:
    """
    Generate output filename based on input filename.
    
    Args:
        input_file: Path to input Q&A JSON file
        
    Returns:
        Output filename for training data
    """
    # Remove '_qa.json' suffix and add '_trainingdata.jsonl'
    base_name = input_file.stem
    if base_name.endswith('_qa'):
        base_name = base_name[:-3]  # Remove '_qa'
    
    return f"{base_name}_trainingdata.jsonl"


def main():
    parser = argparse.ArgumentParser(
        description='Convert Q&A JSON files to training dataset JSONL format'
    )
    
    parser.add_argument(
        'input',
        help='Input Q&A JSON file or directory containing Q&A files'
    )
    parser.add_argument(
        '--output-dir',
        default='.',
        help='Output directory for training files (default: current directory)'
    )
    parser.add_argument(
        '--output-file',
        help='Specific output filename (only for single file input)'
    )
    
    args = parser.parse_args()
    
    input_path = Path(args.input)
    output_dir = Path(args.output_dir)
    
    # Ensure output directory exists
    output_dir.mkdir(parents=True, exist_ok=True)
    
    total_processed = 0
    total_skipped = 0
    files_converted = 0
    
    if input_path.is_file():
        # Process single file
        if args.output_file:
            output_file = output_dir / args.output_file
        else:
            output_filename = generate_output_filename(input_path)
            output_file = output_dir / output_filename
        
        stats = process_qa_file(input_path, output_file)
        total_processed += stats["processed"]
        total_skipped += stats["skipped"]
        if stats["processed"] > 0:
            files_converted += 1
            
    elif input_path.is_dir():
        # Process all Q&A JSON files in directory
        qa_files = list(input_path.glob("*_qa.json"))
        
        if not qa_files:
            log_message(f"No Q&A files (*_qa.json) found in {input_path}")
            return
        
        log_message(f"Found {len(qa_files)} Q&A files to process")
        
        for qa_file in qa_files:
            output_filename = generate_output_filename(qa_file)
            output_file = output_dir / output_filename
            
            stats = process_qa_file(qa_file, output_file)
            total_processed += stats["processed"]
            total_skipped += stats["skipped"]
            if stats["processed"] > 0:
                files_converted += 1
    
    else:
        log_message(f"Input path does not exist: {input_path}")
        return
    
    # Summary
    log_message("=" * 50)
    log_message("Conversion Summary:")
    log_message(f"Files converted: {files_converted}")
    log_message(f"Training examples generated: {total_processed}")
    log_message(f"Files skipped: {total_skipped}")
    log_message(f"Output directory: {output_dir.absolute()}")


if __name__ == "__main__":
    main()
